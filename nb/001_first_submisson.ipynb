{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **[30 Days of ML competition](https://www.kaggle.com/c/30-days-of-ml/overview)**!  In this notebook, you'll learn how to make your first submission.\n",
    "\n",
    "Before getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n",
    "\n",
    "# Step 1: Import helpful libraries\n",
    "\n",
    "We begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:07.109026Z",
     "iopub.status.busy": "2021-08-17T02:17:07.108479Z",
     "iopub.status.idle": "2021-08-17T02:17:08.136843Z",
     "shell.execute_reply": "2021-08-17T02:17:08.135655Z",
     "shell.execute_reply.started": "2021-08-17T02:17:07.108993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Familiar imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For ordinal encoding categorical variables, splitting data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For training random forest model\n",
    "# import lightgbm as lgb\n",
    "# from sklearn.ensemble import lightgbm.\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load the data\n",
    "\n",
    "Next, we'll load the training and test data.  \n",
    "\n",
    "We set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:08.139124Z",
     "iopub.status.busy": "2021-08-17T02:17:08.138680Z",
     "iopub.status.idle": "2021-08-17T02:17:08.144579Z",
     "shell.execute_reply": "2021-08-17T02:17:08.143519Z",
     "shell.execute_reply.started": "2021-08-17T02:17:08.139065Z"
    }
   },
   "outputs": [],
   "source": [
    "def confirm(df):\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:08.147879Z",
     "iopub.status.busy": "2021-08-17T02:17:08.147393Z",
     "iopub.status.idle": "2021-08-17T02:17:12.458788Z",
     "shell.execute_reply": "2021-08-17T02:17:12.457787Z",
     "shell.execute_reply.started": "2021-08-17T02:17:08.147834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../input/30-days-of-ml/train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"../input/30-days-of-ml/test.csv\", index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:12.461492Z",
     "iopub.status.busy": "2021-08-17T02:17:12.461028Z",
     "iopub.status.idle": "2021-08-17T02:17:13.099055Z",
     "shell.execute_reply": "2021-08-17T02:17:13.098011Z",
     "shell.execute_reply.started": "2021-08-17T02:17:12.461444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "confirm(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:13.100788Z",
     "iopub.status.busy": "2021-08-17T02:17:13.100468Z",
     "iopub.status.idle": "2021-08-17T02:17:13.136798Z",
     "shell.execute_reply": "2021-08-17T02:17:13.135766Z",
     "shell.execute_reply.started": "2021-08-17T02:17:13.100758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "y = train['target']\n",
    "features = train.drop(['target'], axis=1)\n",
    "\n",
    "# Preview features\n",
    "# features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare the data\n",
    "\n",
    "Next, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n",
    "\n",
    "In the **[Categorical Variables lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:13.138555Z",
     "iopub.status.busy": "2021-08-17T02:17:13.138230Z",
     "iopub.status.idle": "2021-08-17T02:17:17.457722Z",
     "shell.execute_reply": "2021-08-17T02:17:17.456708Z",
     "shell.execute_reply.started": "2021-08-17T02:17:13.138527Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "object_cols = [col for col in features.columns if 'cat' in col]\n",
    "\n",
    "# ordinal-encode categorical columns\n",
    "X = features.copy()\n",
    "X_test = test.copy()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\n",
    "X_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n",
    "\n",
    "# Preview the ordinal-encoded features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we break off a validation set from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:17.459312Z",
     "iopub.status.busy": "2021-08-17T02:17:17.459016Z",
     "iopub.status.idle": "2021-08-17T02:17:17.591561Z",
     "shell.execute_reply": "2021-08-17T02:17:17.590405Z",
     "shell.execute_reply.started": "2021-08-17T02:17:17.459282Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Predict using XGBoost\n",
    "\n",
    "Use XGBoost instead of random forest. Faster and probably provides better prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:17.593034Z",
     "iopub.status.busy": "2021-08-17T02:17:17.592721Z",
     "iopub.status.idle": "2021-08-17T02:17:17.597471Z",
     "shell.execute_reply": "2021-08-17T02:17:17.596320Z",
     "shell.execute_reply.started": "2021-08-17T02:17:17.593006Z"
    }
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Define the model\n",
    "# my_model_1 = XGBRegressor(random_state=0)\n",
    "\n",
    "# # Fit the model\n",
    "# my_model_1.fit(X_train,y_train) \n",
    "# predictions_1 = my_model_1.predict(X_valid)\n",
    "\n",
    "# # Calculate MAE\n",
    "# mse_1 = mean_squared_error(y_valid, predictions_1, squared=False) \n",
    "\n",
    "\n",
    "# print(\"Mean Squared Error:\" , mse_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.1: Predict using XGBoost with parameters tunning\n",
    "\n",
    "Now, let us try to tune XGBoost to give us better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:17.601072Z",
     "iopub.status.busy": "2021-08-17T02:17:17.600636Z",
     "iopub.status.idle": "2021-08-17T02:17:17.610955Z",
     "shell.execute_reply": "2021-08-17T02:17:17.609777Z",
     "shell.execute_reply.started": "2021-08-17T02:17:17.601025Z"
    }
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# # Define the model\n",
    "# my_model_2 = XGBRegressor(n_estimators=5000, learning_rate=0.05, n_jobs=-1,random_state=0) # Your code here\n",
    "\n",
    "# # Fit the model\n",
    "# my_model_2.fit(X_train,y_train, early_stopping_rounds=10, eval_set=[(X_valid,y_valid)], verbose=False) # Your code here\n",
    "\n",
    "# # Get predictions\n",
    "# predictions_2 = my_model_2.predict(X_valid) # Your code here\n",
    "\n",
    "# # Calculate MAE\n",
    "# mse_2 = mean_squared_error(predictions_2,y_valid) # Your code here\n",
    "\n",
    "# # Uncomment to print MAE\n",
    "# print(\"Mean Squared Error:\" , mse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.2: Predict using LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:17.613482Z",
     "iopub.status.busy": "2021-08-17T02:17:17.613037Z",
     "iopub.status.idle": "2021-08-17T02:17:19.732019Z",
     "shell.execute_reply": "2021-08-17T02:17:19.730739Z",
     "shell.execute_reply.started": "2021-08-17T02:17:17.613436Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgbo\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:19.733760Z",
     "iopub.status.busy": "2021-08-17T02:17:19.733424Z",
     "iopub.status.idle": "2021-08-17T02:17:19.738730Z",
     "shell.execute_reply": "2021-08-17T02:17:19.737438Z",
     "shell.execute_reply.started": "2021-08-17T02:17:19.733729Z"
    }
   },
   "outputs": [],
   "source": [
    "opt_params = {\n",
    "    \"objective\":\"regression\",\n",
    "    \"metric\":\"rmse\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:55.318806Z",
     "iopub.status.busy": "2021-08-17T02:17:55.318411Z",
     "iopub.status.idle": "2021-08-17T02:17:55.323349Z",
     "shell.execute_reply": "2021-08-17T02:17:55.322343Z",
     "shell.execute_reply.started": "2021-08-17T02:17:55.318774Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_train = lgbo.Dataset(X_train,y_train)\n",
    "reg_valid = lgbo.Dataset(X_valid,y_valid,reference=reg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-08-17T02:17:58.800709Z",
     "iopub.status.busy": "2021-08-17T02:17:58.800307Z",
     "iopub.status.idle": "2021-08-17T02:18:51.982830Z",
     "shell.execute_reply": "2021-08-17T02:18:51.981913Z",
     "shell.execute_reply.started": "2021-08-17T02:17:58.800678Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "opt=lgbo.train(opt_params,reg_train,valid_sets = reg_valid,verbose_eval=False,num_boost_round = 5,early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:20:26.326713Z",
     "iopub.status.busy": "2021-08-17T02:20:26.326299Z",
     "iopub.status.idle": "2021-08-17T02:20:26.334151Z",
     "shell.execute_reply": "2021-08-17T02:20:26.333019Z",
     "shell.execute_reply.started": "2021-08-17T02:20:26.326677Z"
    }
   },
   "outputs": [],
   "source": [
    "opt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:20:46.146159Z",
     "iopub.status.busy": "2021-08-17T02:20:46.145746Z",
     "iopub.status.idle": "2021-08-17T02:20:47.064943Z",
     "shell.execute_reply": "2021-08-17T02:20:47.064135Z",
     "shell.execute_reply.started": "2021-08-17T02:20:46.146121Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgbm_parameters = {\n",
    " 'objective': 'regression',\n",
    " 'metric': 'rmse',\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 2.1324554005212664e-05,\n",
    " 'lambda_l2': 7.486212839933644,\n",
    " 'num_leaves': 251,\n",
    " 'feature_fraction': 1.0,\n",
    " 'bagging_fraction': 0.5337542240432858,\n",
    " 'bagging_freq': 3,\n",
    " 'min_child_samples': 20,\n",
    " 'num_iterations': 5,\n",
    "}\n",
    "# early_sr=64\n",
    "\n",
    "\n",
    "lgbm_model = LGBMRegressor(**lgbm_parameters)\n",
    "lgbm_model.fit(X_train, y_train, eval_set = ((X_valid,y_valid)),verbose = -1,categorical_feature=object_cols)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n",
    "\n",
    "# Step 5: Submit to the competition\n",
    "\n",
    "We'll begin by using the trained model to generate predictions, which we'll save to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-17T02:20:55.980266Z",
     "iopub.status.busy": "2021-08-17T02:20:55.979578Z",
     "iopub.status.idle": "2021-08-17T02:20:56.810009Z",
     "shell.execute_reply": "2021-08-17T02:20:56.808990Z",
     "shell.execute_reply.started": "2021-08-17T02:20:55.980207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the model to generate predictions\n",
    "predictions = lgbm_model.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'target': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have run the code cell above, follow the instructions below to submit to the competition:\n",
    "1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n",
    "2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n",
    "3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n",
    "4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n",
    "\n",
    "You have now successfully submitted to the competition!\n",
    "\n",
    "If you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Keep Learning!\n",
    "\n",
    "If you're not sure what to do next, you can begin by trying out more model types!\n",
    "1. If you took the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course, then you learned about **[XGBoost](https://www.kaggle.com/alexisbcook/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n",
    "\n",
    "2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https://www.kaggle.com/svyatoslavsokolov/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
